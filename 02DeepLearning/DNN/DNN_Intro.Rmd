---
title: "Neural Networks and Deep Learning"
author: '[Hui Lin](http://scientistcafe.com) @Netlify</br> </br> Ming Li @Amazon'
date: "`r Sys.Date()`"
output: 
  slidy_presentation: 
    footer: "https://course2019.netlify.com"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

# Types of Neural Network 

<center>
![](../CNN/images/NN_EXP.png){width=80%}
</center>

# Neural Network Application

| Input (x) | Output (y) | Application |
| :---: |  :---:  |  :---: |
| **Home features** | **Price** | **Real Estate** |
| **Ad, user info** | **Click on an Ad? (0/1)** | **Online Advertising** |
| Image | Object (1, ..., 10) | Photo tagging |
| Image, Radar info | Position of other cars | Autonomous driving |
| Audio | Text transcript | Speech recognition |
| English | Chinese | Machine translation |
| Voice  | Voice | Human computer conversation |

# Why is deep learning taking off?

- Scale, scale, scale

![](images/dl_driver1.png){width=90%}

# Why is deep learning taking off?

![](images/dl_driver2.png){width=90%}

# Why is deep learning taking off?

![](images/dl_driver3.png){width=90%}

# Why is deep learning taking off?

![](images/dl_driver4.png){width=90%}

# Why is deep learning taking off?

![](images/dl_driver5.png){width=90%}

# Why is deep learning taking off?

![](images/dl_driver6.png){width=90%}

# Logistic Regression as a Neural Network

- m training samples: $\{(x^{(1)}, y^{(1)}),(x^{(2)}, y^{(2)}),...,(x^{(m)}, y^{(m)})\}$

$$X=\left[\begin{array}{cccc}
x_{1}^{(1)} & x_{1}^{(2)} & \dotsb & x_{1}^{(m)}\\
x_{2}^{(1)} & x_{2}^{(2)} & \dotsb & x_{2}^{(m)}\\
\vdots & \vdots & \vdots & \vdots\\
x_{n_{x}}^{(1)} & x_{n_{x}}^{(2)} & \dots & x_{n_{x}}^{(m)}
\end{array}\right]\in\mathbb{R}^{n_{x}\times m}$$

$$y=[y^{(1)},y^{(2)},\dots,y^{(m)}] \in \mathbb{R}^{1 \times m}$$

$\hat{y}^{(i)} = \sigma(w^Tx^{(i)} + b)$ where $\sigma(z) = \frac{1}{1+e^{-z}}$


# Logistic Regression as a Neural Network

- m training samples: $\{(x^{(1)}, y^{(1)}),(x^{(2)}, y^{(2)}),...,(x^{(m)}, y^{(m)})\}$

$$X=\left[\begin{array}{cccc}
x_{1}^{(1)} & x_{1}^{(2)} & \dotsb & x_{1}^{(m)}\\
x_{2}^{(1)} & x_{2}^{(2)} & \dotsb & x_{2}^{(m)}\\
\vdots & \vdots & \vdots & \vdots\\
x_{n_{x}}^{(1)} & x_{n_{x}}^{(2)} & \dots & x_{n_{x}}^{(m)}
\end{array}\right]\in\mathbb{R}^{n_{x}\times m}$$

$$y=[y^{(1)},y^{(2)},\dots,y^{(m)}] \in \mathbb{R}^{1 \times m}$$

$\hat{y}^{(i)} = \sigma(w^Tx^{(i)} + b)$ where $\sigma(z) = \frac{1}{1+e^{-z}}$

- Loss function: $L(\hat{y},y) = -ylog(\hat{y})-(1-y)log(1-\hat{y})$
- Cost function: $J(w,b)=\frac{1}{m} \Sigma_{i=1}^m L(\hat{y}^{(i)}, y^{(i)}) = \frac{1}{m} \Sigma_{i=1}^{m} \{\ -y^{(i)}log(\hat{y}^{(i)})-(1-y^{(i)})log(1-\hat{y}^{(i)}) \}$
- Goal: Find $w,b$ that mininizes $J(w,b)$
    - $w := w - \alpha \frac{\partial J}{\partial w}$
    - $b := b - \alpha \frac{\partial J}{\partial b}$

# Neural Network:  0 layer neural network

<center>
![](images/dnn0.png){width=60%}
</center>

# Neural Network: 1 layer neural network

<center>
![](images/dnn1.png){width=60%}
</center>

# Neural Network: 1 layer neural network

<center>
![](images/dnn_math1.png){width=80%}
</center>

# Neural Network: 1 layer neural network

<center>
![](images/dnn_math2.png){width=80%}
</center>

# Neural Network: 1 layer neural network

<center>
![](images/dnn_math3.png){width=80%}
</center>

# Across m samples

<center>
![](images/dnn_math4.png){width=80%}
</center>

# Across m samples

<center>
![](images/dnn_math5.png){width=80%}
</center>

# Activation functions

<center>
![](images/activation.png){width=80%}
</center>

# Activation functions

- Intermediate layers
    - Relu (i.e. rectified linear unit) is usually a good choice which has the following good properties:   
(1) fast computation;   
(2) non-linear;  
(3) reduced likelihood of the gradient to vanish;   
(4) Unconstrained response
    - Sigmoid, studied in the past, not as good as Relu in deep learning, due to the gradient vanishing problem when there are many layers
    - hyperbolic tangent function (tanh)

- Last layer which connects to the output
    - Binary classification: sigmoid with binary cross entropy as loss function
    - Multiple class, single-label classification: softmax with categorical cross entropy for loss function
    - Continuous responses: identity function (i.e. y = x)

# Optimization Methods

- Mini-batch Stochastic Gradient Descent (SGD)
    - Use a small segment of data (i.e. 128 or 256) to update the SGD parameters:  where  is the learning rate which is a hyper parameter that can be changed
    - Gradients are efficiently calculated using backpropagation method
    - When the entire dataset are used to updated the SGD, it is called one epoch and multiple epochs are needed to run to reach convergence
    - An updated version with ‘momentum’ for quick convergence

# Deal With Overfitting

- Huge number of parameters, even with large amount of training data, there is a potential of overfitting 
    - Overfitting due to size of the NN (i.e. total number of parameters)
    - Overfitting due to using the training data for too many epochs

- Solution for overfitting due to NN size
    - Dropout: randomly dropout some proportion (such as 0.3 or 0.5) of nodes at each layer, which is similar to random forest concept 
    - Using L1 or L2 regularization in the activation function at each layer 

- Solution for overfitting due to using too many epochs
    - Run NN with large number of epochs to reach overfitting region through cross validation from training/validation vs. epoch curve
    - Choose the model with number of epochs that have the minimum validation accuracy as the final NN model
    - The optimal number for epoch is determined by when the model is not overfitted (i.e. validation accuracy reaches the best performance)